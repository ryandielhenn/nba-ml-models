{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ef8090-618d-4c12-a0f6-80bd3987fe81",
   "metadata": {},
   "source": [
    "# NBA Player Stats Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95413010-0205-4772-9b24-546bc9d401cc",
   "metadata": {},
   "source": [
    "To start we can download the dataset using kagglehub and get an overview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1293371-7743-4462-951e-16c16057552e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Path to dataset files: /Users/momokaaung/.cache/kagglehub/datasets/eduardopalmieri/nba-player-stats-season-2425/versions/37\n",
      "\n",
      "Available CSV files: ['database_24_25.csv']\n",
      "\n",
      "================================================================================\n",
      "DATASET OVERVIEW\n",
      "================================================================================\n",
      "\n",
      "Dataset shape: (16512, 25)\n",
      "\n",
      "Column names:\n",
      "['Player', 'Tm', 'Opp', 'Res', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'GmSc', 'Data']\n",
      "\n",
      "First few rows:\n",
      "          Player   Tm  Opp Res     MP  FG  FGA    FG%  3P  3PA  ...  DRB  TRB  \\\n",
      "0   Jayson Tatum  BOS  NYK   W  30.30  14   18  0.778   8   11  ...    4    4   \n",
      "1  Anthony Davis  LAL  MIN   W  37.58  11   23  0.478   1    3  ...   13   16   \n",
      "2  Derrick White  BOS  NYK   W  26.63   8   13  0.615   6   10  ...    3    3   \n",
      "3   Jrue Holiday  BOS  NYK   W  30.52   7    9  0.778   4    6  ...    2    4   \n",
      "4  Miles McBride  NYK  BOS   L  25.85   8   10  0.800   4    5  ...    0    0   \n",
      "\n",
      "   AST  STL  BLK  TOV  PF  PTS  GmSc        Data  \n",
      "0   10    1    1    1   1   37  38.1  2024-10-22  \n",
      "1    4    1    3    1   1   36  34.0  2024-10-22  \n",
      "2    4    1    0    0   1   24  22.4  2024-10-22  \n",
      "3    4    1    0    0   2   18  19.5  2024-10-22  \n",
      "4    2    0    0    1   1   22  17.8  2024-10-22  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "\n",
      "================================================================================\n",
      "DATA QUALITY CHECK\n",
      "================================================================================\n",
      "\n",
      "Missing values:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download dataset\n",
    "print(\"Downloading dataset...\")\n",
    "path = kagglehub.dataset_download(\"eduardopalmieri/nba-player-stats-season-2425\")\n",
    "print(f\"Path to dataset files: {path}\")\n",
    "\n",
    "# Load the dataset\n",
    "import os\n",
    "csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "print(f\"\\nAvailable CSV files: {csv_files}\")\n",
    "\n",
    "# Load the first CSV file (adjust if needed)\n",
    "df = pd.read_csv(os.path.join(path, csv_files[0]))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\\n{df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\\n{df.head()}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "missing_values = df.isnull().sum()\n",
    "print(f\"\\nMissing values:\\n{missing_values[missing_values > 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e12c0-94ab-4ae5-81c7-97247a120a89",
   "metadata": {},
   "source": [
    "There aren't any missing values in the dataset which is good news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31c77335-7152-47c9-aa89-8a4e3b54f17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target variable: FG\n",
      "\n",
      "Feature variables (19 total):\n",
      "['Tm', 'Opp', 'Res', 'MP', 'FGA', '3P', '3PA', '3P%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF']\n",
      "\n",
      "Numeric features: ['MP', 'FGA', '3P', '3PA', '3P%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF']\n",
      "\n",
      "Categorical features: ['Tm', 'Opp', 'Res']\n",
      "\n",
      "Rows before dropping NaN: 16512\n",
      "Rows after dropping NaN: 16512\n",
      "\n",
      "Final dataset shape: X=(16512, 16), y=(16512,)\n"
     ]
    }
   ],
   "source": [
    "# Define target variable\n",
    "target_col = 'FG'  # What we're predicting\n",
    "\n",
    "# Define features - use ALL columns except:\n",
    "# - The target itself (FG)\n",
    "# - Non-predictive columns (Player name, Date)\n",
    "exclude_cols = ['FG', 'Player', 'Data', 'FG%', 'PTS', 'GmSc']  # columns to exclude\n",
    "\n",
    "# Get all feature columns\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nTarget variable: {target_col}\")\n",
    "print(f\"\\nFeature variables ({len(feature_cols)} total):\")\n",
    "print(feature_cols)\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df[feature_cols].select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric features: {numeric_cols}\")\n",
    "print(f\"\\nCategorical features: {categorical_cols}\")\n",
    "\n",
    "# For now, let's use only numeric features (we can encode categoricals later if needed)\n",
    "X = df[numeric_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"\\nRows before dropping NaN: {len(X)}\")\n",
    "valid_indices = X.notna().all(axis=1) & y.notna()\n",
    "X = X[valid_indices]\n",
    "y = y[valid_indices]\n",
    "print(f\"Rows after dropping NaN: {len(X)}\")\n",
    "\n",
    "# Handle any infinite values (from division by zero, etc.)\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.dropna()\n",
    "y = y[X.index]\n",
    "\n",
    "print(f\"\\nFinal dataset shape: X={X.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbfb4d9-09de-49bb-92c2-1fcd1a162438",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "- Linear Regression\n",
    "- Random Forest\n",
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fcef13e-2605-4e56-9ce7-1a984b5f4c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL TRAINING\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Linear Regression\n",
      "--------------------------------------------------------------------------------\n",
      "RMSE: 1.0880\n",
      "MAE: 0.7948\n",
      "R²: 0.8874\n",
      "\n",
      "Feature Coefficients:\n",
      "  Tm: 0.1313\n",
      "  Opp: 3.2700\n",
      "  Res: 1.5630\n",
      "  MP: -1.8575\n",
      "  FGA: -0.0442\n",
      "  3P: -0.1396\n",
      "  3PA: 0.1424\n",
      "  3P%: 0.0408\n",
      "  FT: 3766820443346.9692\n",
      "  FTA: 7517534931008.0820\n",
      "  FT%: -9582703120100.6758\n",
      "  ORB: -0.0379\n",
      "  DRB: 0.0194\n",
      "  TRB: 0.0453\n",
      "  AST: -0.0101\n",
      "  STL: -0.0159\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Random Forest\n",
      "--------------------------------------------------------------------------------\n",
      "RMSE: 1.1613\n",
      "MAE: 0.8345\n",
      "R²: 0.8717\n",
      "\n",
      "Feature Importances:\n",
      "  Tm: 0.0242\n",
      "  Opp: 0.8051\n",
      "  Res: 0.0032\n",
      "  MP: 0.0336\n",
      "  FGA: 0.0554\n",
      "  3P: 0.0055\n",
      "  3PA: 0.0069\n",
      "  3P%: 0.0063\n",
      "  FT: 0.0063\n",
      "  FTA: 0.0086\n",
      "  FT%: 0.0093\n",
      "  ORB: 0.0101\n",
      "  DRB: 0.0057\n",
      "  TRB: 0.0048\n",
      "  AST: 0.0076\n",
      "  STL: 0.0075\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Gradient Boosting\n",
      "--------------------------------------------------------------------------------\n",
      "RMSE: 1.1082\n",
      "MAE: 0.8085\n",
      "R²: 0.8832\n",
      "\n",
      "Feature Importances:\n",
      "  Tm: 0.0157\n",
      "  Opp: 0.8855\n",
      "  Res: 0.0027\n",
      "  MP: 0.0291\n",
      "  FGA: 0.0584\n",
      "  3P: 0.0002\n",
      "  3PA: 0.0019\n",
      "  3P%: 0.0004\n",
      "  FT: 0.0003\n",
      "  FTA: 0.0013\n",
      "  FT%: 0.0033\n",
      "  ORB: 0.0005\n",
      "  DRB: 0.0001\n",
      "  TRB: 0.0002\n",
      "  AST: 0.0002\n",
      "  STL: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Define models with their configurations\n",
    "models = {\n",
    "    'Linear Regression': {\n",
    "        'model': LinearRegression(),\n",
    "        'use_scaled': True,\n",
    "        'has_coef': True\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        'use_scaled': False,\n",
    "        'has_coef': False\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        'use_scaled': False,\n",
    "        'has_coef': False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train and evaluate all models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "for model_name, config in models.items():\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Select appropriate training data\n",
    "    X_train_use = X_train_scaled if config['use_scaled'] else X_train\n",
    "    X_test_use = X_test_scaled if config['use_scaled'] else X_test\n",
    "    \n",
    "    # Train model\n",
    "    model = config['model']\n",
    "    model.fit(X_train_use, y_train)\n",
    "    y_pred = model.predict(X_test_use)\n",
    "    \n",
    "    # Calculate and store metrics\n",
    "    results[model_name] = {\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'R²': r2_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Print metrics\n",
    "    for metric, value in results[model_name].items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Print coefficients or feature importances\n",
    "    if config['has_coef']:\n",
    "        print(\"\\nFeature Coefficients:\")\n",
    "        for feature, coef in zip(feature_cols, model.coef_):\n",
    "            print(f\"  {feature}: {coef:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nFeature Importances:\")\n",
    "        for feature, importance in zip(feature_cols, model.feature_importances_):\n",
    "            print(f\"  {feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9f655-1ffe-4a7b-84f6-f53851113ab2",
   "metadata": {},
   "source": [
    "## Summary of model results and finding the best model\n",
    "Three ways to measure how good your predictions are:\n",
    "\n",
    "1. R² (R-squared) - \"How well does my model fit?\"\n",
    "   - Score: 0 to 1 (1 is perfect, higher is better)\n",
    "   - Think of it as: A percentage score for your model\n",
    "   - Example: R² = 0.85 means your model is 85% accurate at explaining the data\n",
    "   - Simple rule: Above 0.7 is usually good, above 0.9 is excellent\n",
    "   \n",
    "2. RMSE (Root Mean Squared Error) - \"How far off am I, on average?\"\n",
    "   - Score: Lower is better (0 is perfect)\n",
    "   - Think of it as: Your typical prediction error in real units\n",
    "   - Example: RMSE = 2.5 means you're off by about 2-3 field goals per prediction\n",
    "   - Simple rule: Compare to your target values - if predicting 5-10 FG, \n",
    "     an RMSE of 0.5 is great, but 5.0 is terrible\n",
    "   - Important: Punishes big mistakes harder (being off by 10 is worse than \n",
    "     being off by 1 ten times)\n",
    "   \n",
    "3. MAE (Mean Absolute Error) - \"What's my average mistake?\"\n",
    "   - Score: Lower is better (0 is perfect)\n",
    "   - Think of it as: The average size of your errors\n",
    "   - Example: MAE = 2.0 means on average you're wrong by 2 field goals\n",
    "   - Simple rule: Same as RMSE - lower is better, compare to your data range\n",
    "   - Important: Treats all mistakes equally (being off by 10 once = being off \n",
    "     by 1 ten times)\n",
    "\n",
    "Which should you use?\n",
    "- R²: Quick overall grade (like a test score)\n",
    "- RMSE: When big mistakes are really bad (predicting medical dosages)\n",
    "- MAE: When you just want the typical error size (most common cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df1d3c2f-61a2-4d50-92b9-71675dd88a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS\n",
      "================================================================================\n",
      "                       RMSE       MAE        R²\n",
      "Linear Regression  1.088000  0.794814  0.887364\n",
      "Random Forest      1.161312  0.834471  0.871673\n",
      "Gradient Boosting  1.108159  0.808517  0.883151\n",
      "\n",
      "Best Model (by R²): Linear Regression\n",
      "Best Model (by RMSE): Linear Regression\n",
      "Best Model (by MAE): Linear Regression\n"
     ]
    }
   ],
   "source": [
    "# Summary of results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY OF RESULTS\")\n",
    "print(\"=\"*80)\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Pick best by R² (maximize)\n",
    "best_model_r2 = results_df['R²'].idxmax()\n",
    "\n",
    "# Pick best by RMSE (minimize)\n",
    "best_model_rmse = results_df['RMSE'].idxmin()\n",
    "\n",
    "# Pick best by MAE (minimize)\n",
    "best_model_mae = results_df['MAE'].idxmin()\n",
    "\n",
    "print(f\"\\nBest Model (by R²): {best_model_r2}\")\n",
    "print(f\"Best Model (by RMSE): {best_model_rmse}\")\n",
    "print(f\"Best Model (by MAE): {best_model_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bdbdf31f-6697-4046-b2a9-bd6f8a1255d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "SECOND MODELING TASK: PREDICTING FGA (Field Goal Attempts)\n",
      "================================================================================\n",
      "\n",
      "Final dataset for FGA: X=(16512, 15), y=(16512,)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# SECOND PREDICTION TASK: Predict FGA (Field Goal Attempts)\n",
    "# ================================================================================\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SECOND MODELING TASK: PREDICTING FGA (Field Goal Attempts)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define new target\n",
    "target_col = 'FGA'\n",
    "\n",
    "# Exclude columns related to FGA or non-predictive ones\n",
    "exclude_cols = ['FGA', 'FG', 'Player', 'Data', 'FG%', 'PTS', 'GmSc']\n",
    "\n",
    "# Select features\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "X = df[numeric_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# Clean data\n",
    "valid_indices = X.notna().all(axis=1) & y.notna()\n",
    "X = X[valid_indices]\n",
    "y = y.loc[X.index]\n",
    "X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "print(f\"\\nFinal dataset for FGA: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b12b2c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL TRAINING\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Linear Regression\n",
      "--------------------------------------------------------------------------------\n",
      "RMSE: 2.7505\n",
      "MAE: 2.0328\n",
      "R²: 0.7918\n",
      "\n",
      "Feature Coefficients:\n",
      "  MP: 1.9253\n",
      "  3P: -0.0991\n",
      "  3PA: 2.9083\n",
      "  3P%: -0.1319\n",
      "  FT: 0.0319\n",
      "  FTA: 1.0524\n",
      "  FT%: -0.0552\n",
      "  ORB: -8198447534941.5078\n",
      "  DRB: -16361840616219.9512\n",
      "  TRB: 20856658806721.3750\n",
      "  AST: 0.4037\n",
      "  STL: 0.0829\n",
      "  BLK: 0.0427\n",
      "  TOV: 0.3908\n",
      "  PF: -0.1837\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Random Forest\n",
      "--------------------------------------------------------------------------------\n",
      "RMSE: 2.8018\n",
      "MAE: 2.0417\n",
      "R²: 0.7840\n",
      "\n",
      "Feature Importances:\n",
      "  MP: 0.6348\n",
      "  3P: 0.0057\n",
      "  3PA: 0.1662\n",
      "  3P%: 0.0109\n",
      "  FT: 0.0144\n",
      "  FTA: 0.0308\n",
      "  FT%: 0.0110\n",
      "  ORB: 0.0162\n",
      "  DRB: 0.0153\n",
      "  TRB: 0.0200\n",
      "  AST: 0.0222\n",
      "  STL: 0.0113\n",
      "  BLK: 0.0084\n",
      "  TOV: 0.0176\n",
      "  PF: 0.0152\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Gradient Boosting\n",
      "--------------------------------------------------------------------------------\n",
      "RMSE: 2.7241\n",
      "MAE: 1.9857\n",
      "R²: 0.7958\n",
      "\n",
      "Feature Importances:\n",
      "  MP: 0.6794\n",
      "  3P: 0.0009\n",
      "  3PA: 0.2250\n",
      "  3P%: 0.0012\n",
      "  FT: 0.0041\n",
      "  FTA: 0.0468\n",
      "  FT%: 0.0011\n",
      "  ORB: 0.0100\n",
      "  DRB: 0.0010\n",
      "  TRB: 0.0093\n",
      "  AST: 0.0105\n",
      "  STL: 0.0006\n",
      "  BLK: 0.0007\n",
      "  TOV: 0.0087\n",
      "  PF: 0.0007\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# MODEL TRAINING\n",
    "# ================================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fga_results = {}\n",
    "\n",
    "for model_name, config in models.items():\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    X_train_use = X_train_scaled if config['use_scaled'] else X_train\n",
    "    X_test_use = X_test_scaled if config['use_scaled'] else X_test\n",
    "\n",
    "    model = config['model']\n",
    "    model.fit(X_train_use, y_train)\n",
    "    y_pred = model.predict(X_test_use)\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    fga_results[model_name] = {'RMSE': rmse, 'MAE': mae, 'R²': r2}\n",
    "\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "    # Coefficients or feature importances\n",
    "    if config['has_coef'] and hasattr(model, 'coef_'):\n",
    "        print(\"\\nFeature Coefficients:\")\n",
    "        for feature, coef in zip(numeric_cols, model.coef_):\n",
    "            print(f\"  {feature}: {coef:.4f}\")\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        print(\"\\nFeature Importances:\")\n",
    "        for feature, importance in zip(numeric_cols, model.feature_importances_):\n",
    "            print(f\"  {feature}: {importance:.4f}\")\n",
    "    else:\n",
    "        print(\"\\n(No coefficients or feature importances available for this model.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f194b655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS\n",
      "================================================================================\n",
      "                          RMSE         MAE        R²\n",
      "Linear Regression     2.750458    2.032782  0.791823\n",
      "Random Forest         2.801822    2.041688  0.783975\n",
      "Gradient Boosting     2.724104    1.985679  0.795793\n",
      "\n",
      "Best Model (by R²): Gradient Boosting\n",
      "Best Model (by RMSE): Gradient Boosting\n",
      "Best Model (by MAE): Gradient Boosting\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# SUMMARY OF RESULTS\n",
    "# ================================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY OF RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fga_results_df = pd.DataFrame(fga_results).T\n",
    "print(f\"{'':<20s}{'RMSE':>10s}{'MAE':>12s}{'R²':>10s}\")\n",
    "for model_name, row in fga_results_df.iterrows():\n",
    "    print(f\"{model_name:<20s}{row['RMSE']:>10.6f}{row['MAE']:>12.6f}{row['R²']:>10.6f}\")\n",
    "\n",
    "# Identify best models\n",
    "best_model_r2 = fga_results_df['R²'].idxmax()\n",
    "best_model_rmse = fga_results_df['RMSE'].idxmin()\n",
    "best_model_mae = fga_results_df['MAE'].idxmin()\n",
    "\n",
    "print(f\"\\nBest Model (by R²): {best_model_r2}\")\n",
    "print(f\"Best Model (by RMSE): {best_model_rmse}\")\n",
    "print(f\"Best Model (by MAE): {best_model_mae}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
