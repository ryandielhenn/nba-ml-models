{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# NBA Player Stats Prediction - Refactored\n",
    "\n",
    "**Team Members:** Ryan, Momoka, Jesus, Angel, Harshil   \n",
    "**Course:** CS4661 - Introduction to Data Science  \n",
    "**Objective:** Predict NBA player statistics using machine learning\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook demonstrates a complete machine learning pipeline for predicting NBA player statistics:\n",
    "- **Target Variables:** PTS (total points scored) and team win classifiction\n",
    "- **Models:** Linear Regression, Random Forest, Gradient Boosting\n",
    "- **Approach:** Modular, reusable functions for scalability and maintainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions",
   "metadata": {},
   "source": [
    "## 2. Reusable Functions\n",
    "\n",
    "These functions eliminate code duplication and make the pipeline modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "helper_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nba_data():\n",
    "    \"\"\"\n",
    "    Download and load NBA player stats dataset from Kaggle.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Raw dataset\n",
    "    \"\"\"\n",
    "    print(\"Downloading dataset...\")\n",
    "    path = kagglehub.dataset_download(\"eduardopalmieri/nba-player-stats-season-2425\")\n",
    "    print(f\"Path to dataset files: {path}\")\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "    print(f\"\\nAvailable CSV files: {csv_files}\")\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(path, csv_files[0]))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATASET OVERVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    print(f\"\\nColumn names:\\n{df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\\n{df.head()}\")\n",
    "    \n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(f\"\\nMissing values:\\n{missing_values[missing_values > 0]}\")\n",
    "    else:\n",
    "        print(\"\\nNo missing values found!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_features(df, target_col, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Prepare features and target variable for modeling.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with raw data\n",
    "        target_col: Name of target variable column\n",
    "        exclude_cols: List of columns to exclude (default: auto-detected)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X, y, feature_names)\n",
    "    \"\"\"\n",
    "    if exclude_cols is None:\n",
    "        # Auto-detect columns to exclude\n",
    "        exclude_cols = [target_col, 'Player', 'Data', 'FG%', 'PTS', 'GmSc']\n",
    "        \n",
    "        # Conditionally exclude based on target\n",
    "        if target_col in ['FG', 'FGA']:\n",
    "            exclude_cols.extend(['PTS', 'GmSc'])\n",
    "        elif target_col == 'PTS':\n",
    "            exclude_cols.extend(['GmSc', 'FG'])\n",
    "        elif target_col == 'GmSc':\n",
    "            exclude_cols.extend(['PTS'])\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Select only numeric features for now\n",
    "    numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nTarget variable: {target_col}\")\n",
    "    print(f\"Feature variables ({len(numeric_cols)} total): {numeric_cols}\")\n",
    "    \n",
    "    # Create feature matrix and target vector\n",
    "    X = df[numeric_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # Clean data\n",
    "    valid_indices = X.notna().all(axis=1) & y.notna()\n",
    "    X = X[valid_indices]\n",
    "    y = y[valid_indices]\n",
    "    \n",
    "    # Handle infinite values\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    y = y[X.index]\n",
    "    \n",
    "    print(f\"Final dataset shape: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    return X, y, numeric_cols\n",
    "\n",
    "\n",
    "def create_model_configs():\n",
    "    \"\"\"\n",
    "    Create model configurations for training.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model configurations\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'Linear Regression': {\n",
    "            'model': LinearRegression(),\n",
    "            'use_scaled': True,\n",
    "            'has_coef': True\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'use_scaled': False,\n",
    "            'has_coef': False\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'model': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "            'use_scaled': False,\n",
    "            'has_coef': False\n",
    "        }\n",
    "    }\n",
    "    return models\n",
    "\n",
    "\n",
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test, feature_cols, target_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate all models for a given target variable.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        X_test: Test features\n",
    "        y_train: Training target\n",
    "        y_test: Test target\n",
    "        feature_cols: List of feature column names\n",
    "        target_name: Name of target variable (for display)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results for each model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"MODEL TRAINING FOR {target_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    models = create_model_configs()\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize scaler once for all models that need it\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    for model_name, config in models.items():\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Select scaled or unscaled data based on model requirements\n",
    "        X_train_use = X_train_scaled if config['use_scaled'] else X_train\n",
    "        X_test_use = X_test_scaled if config['use_scaled'] else X_test\n",
    "        \n",
    "        # Train model\n",
    "        model = config['model']\n",
    "        model.fit(X_train_use, y_train)\n",
    "        y_pred = model.predict(X_test_use)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R²': r2\n",
    "        }\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"R²: {r2:.4f}\")\n",
    "        \n",
    "        # Print coefficients or feature importances\n",
    "        if config['has_coef'] and hasattr(model, 'coef_'):\n",
    "            print(\"\\nFeature Coefficients:\")\n",
    "            for feature, coef in zip(feature_cols, model.coef_):\n",
    "                print(f\"  {feature}: {coef:.4f}\")\n",
    "        elif hasattr(model, 'feature_importances_'):\n",
    "            print(\"\\nFeature Importances:\")\n",
    "            for feature, importance in zip(feature_cols, model.feature_importances_):\n",
    "                print(f\"  {feature}: {importance:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def summarize_results(results, target_name):\n",
    "    \"\"\"\n",
    "    Print summary of model results.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of model results\n",
    "        target_name: Name of target variable\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"SUMMARY OF RESULTS FOR {target_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(f\"\\n{'':<20s}{'RMSE':>10s}{'MAE':>12s}{'R²':>10s}\")\n",
    "    for model_name, row in results_df.iterrows():\n",
    "        print(f\"{model_name:<20s}{row['RMSE']:>10.6f}{row['MAE']:>12.6f}{row['R²']:>10.6f}\")\n",
    "    \n",
    "    # Identify best models\n",
    "    best_model_r2 = results_df['R²'].idxmax()\n",
    "    best_model_rmse = results_df['RMSE'].idxmin()\n",
    "    best_model_mae = results_df['MAE'].idxmin()\n",
    "    \n",
    "    print(f\"\\nBest Model (by R²): {best_model_r2}\")\n",
    "    print(f\"Best Model (by RMSE): {best_model_rmse}\")\n",
    "    print(f\"Best Model (by MAE): {best_model_mae}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def predict_target(df, target_col, test_size=0.4, random_state=42):\n",
    "    \"\"\"\n",
    "    Complete pipeline for predicting a target variable.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with data\n",
    "        target_col: Target variable to predict\n",
    "        test_size: Proportion of data for testing\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results for all models\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(f\"# PREDICTION PIPELINE FOR: {target_col}\")\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    # Prepare features\n",
    "    X, y, feature_cols = prepare_features(df, target_col)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    results = train_and_evaluate_models(\n",
    "        X_train, X_test, y_train, y_test, feature_cols, target_col\n",
    "    )\n",
    "    \n",
    "    # Summarize results\n",
    "    results_df = summarize_results(results, target_col)\n",
    "    \n",
    "    return results, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b12d9db2-1a11-4ed6-8f2c-006e8c1a0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(X_train, y_train, model_type='random_forest'):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning using GridSearchCV or RandomizedSearchCV.\n",
    "    Cross-validation is handled automatically by these methods.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        model_type: Type of model to tune ('random_forest', 'gradient_boosting', 'xgboost', 'lightgbm')\n",
    "    \n",
    "    Returns:\n",
    "        GridSearchCV object containing:\n",
    "            - best_estimator_: The best trained model\n",
    "            - best_params_: The best parameter combination\n",
    "            - best_score_: The best cross-validation score\n",
    "            - cv_results_: Detailed results for all parameter combinations\n",
    "    \n",
    "    Usage:\n",
    "        grid_search = tune_hyperparameters(X_train, y_train, 'random_forest')\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    TODO (Jesus): Implement this function with:\n",
    "        1. Define param_grid for each model_type\n",
    "        2. Create GridSearchCV with cv=5\n",
    "        3. Fit and return the grid_search object\n",
    "    \"\"\"\n",
    "    # TODO: Add imports at top of notebook if needed\n",
    "    # from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "    \n",
    "    # TODO: Define parameter grids for different models\n",
    "    # param_grids = {\n",
    "    #     'random_forest': {\n",
    "    #         'n_estimators': [50, 100, 200],\n",
    "    #         'max_depth': [10, 20, None],\n",
    "    #         'min_samples_split': [2, 5, 10]\n",
    "    #     },\n",
    "    #     'gradient_boosting': {\n",
    "    #         'n_estimators': [50, 100, 200],\n",
    "    #         'learning_rate': [0.01, 0.1, 0.2],\n",
    "    #         'max_depth': [3, 5, 7]\n",
    "    #     }\n",
    "    # }\n",
    "    \n",
    "    # TODO: Create GridSearchCV and fit\n",
    "    # grid_search = GridSearchCV(\n",
    "    #     estimator=...,\n",
    "    #     param_grid=param_grids[model_type],\n",
    "    #     cv=5,  # 5-fold cross-validation (automatic)\n",
    "    #     scoring='r2',\n",
    "    #     n_jobs=-1,  # Use all CPU cores\n",
    "    #     verbose=1\n",
    "    # )\n",
    "    # grid_search.fit(X_train, y_train)\n",
    "    # return grid_search\n",
    "    \n",
    "    raise NotImplementedError(\"Jesus: Implement hyperparameter tuning here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Path to dataset files: /Users/ryan/.cache/kagglehub/datasets/eduardopalmieri/nba-player-stats-season-2425/versions/37\n",
      "\n",
      "Available CSV files: ['database_24_25.csv']\n",
      "\n",
      "================================================================================\n",
      "DATASET OVERVIEW\n",
      "================================================================================\n",
      "\n",
      "Dataset shape: (16512, 25)\n",
      "\n",
      "Column names:\n",
      "['Player', 'Tm', 'Opp', 'Res', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'GmSc', 'Data']\n",
      "\n",
      "First few rows:\n",
      "          Player   Tm  Opp Res     MP  FG  FGA    FG%  3P  3PA  ...  DRB  TRB  \\\n",
      "0   Jayson Tatum  BOS  NYK   W  30.30  14   18  0.778   8   11  ...    4    4   \n",
      "1  Anthony Davis  LAL  MIN   W  37.58  11   23  0.478   1    3  ...   13   16   \n",
      "2  Derrick White  BOS  NYK   W  26.63   8   13  0.615   6   10  ...    3    3   \n",
      "3   Jrue Holiday  BOS  NYK   W  30.52   7    9  0.778   4    6  ...    2    4   \n",
      "4  Miles McBride  NYK  BOS   L  25.85   8   10  0.800   4    5  ...    0    0   \n",
      "\n",
      "   AST  STL  BLK  TOV  PF  PTS  GmSc        Data  \n",
      "0   10    1    1    1   1   37  38.1  2024-10-22  \n",
      "1    4    1    3    1   1   36  34.0  2024-10-22  \n",
      "2    4    1    0    0   1   24  22.4  2024-10-22  \n",
      "3    4    1    0    0   2   18  19.5  2024-10-22  \n",
      "4    2    0    0    1   1   22  17.8  2024-10-22  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "\n",
      "No missing values found!\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (only need to do this once!)\n",
    "df = load_nba_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predict_fg",
   "metadata": {},
   "source": [
    "## 4. Predict Points Scored (PTS)\n",
    "\n",
    "Points scored (PTS) represents the number of successful points made by a player in a game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fg_prediction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# PREDICTION PIPELINE FOR: PTS\n",
      "################################################################################\n",
      "\n",
      "Target variable: PTS\n",
      "Feature variables (16 total): ['MP', 'FGA', '3P', '3PA', '3P%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF']\n",
      "Final dataset shape: X=(16512, 16), y=(16512,)\n",
      "\n",
      "Train set: 9907 samples\n",
      "Test set: 6605 samples\n",
      "\n",
      "================================================================================\n",
      "MODEL TRAINING FOR PTS\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Linear Regression\n",
      "--------------------------------------------------------------------------------\n",
      "RMSE: 2.1743\n",
      "MAE: 1.5885\n",
      "R²: 0.9391\n",
      "\n",
      "Feature Coefficients:\n",
      "  MP: 0.2254\n",
      "  FGA: 6.5157\n",
      "  3P: 4.6443\n",
      "  3PA: -3.6856\n",
      "  3P%: -0.0842\n",
      "  FT: 1.9715\n",
      "  FTA: 0.2723\n",
      "  FT%: 0.0838\n",
      "  ORB: -0.0956\n",
      "  DRB: 0.1303\n",
      "  TRB: 0.0647\n",
      "  AST: -0.0781\n",
      "  STL: 0.0363\n",
      "  BLK: 0.0855\n",
      "  TOV: -0.0170\n",
      "  PF: -0.0298\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Random Forest\n",
      "--------------------------------------------------------------------------------\n",
      "RMSE: 2.3868\n",
      "MAE: 1.7168\n",
      "R²: 0.9266\n",
      "\n",
      "Feature Importances:\n",
      "  MP: 0.0134\n",
      "  FGA: 0.7884\n",
      "  3P: 0.0162\n",
      "  3PA: 0.0132\n",
      "  3P%: 0.0634\n",
      "  FT: 0.0325\n",
      "  FTA: 0.0297\n",
      "  FT%: 0.0076\n",
      "  ORB: 0.0037\n",
      "  DRB: 0.0053\n",
      "  TRB: 0.0055\n",
      "  AST: 0.0058\n",
      "  STL: 0.0035\n",
      "  BLK: 0.0030\n",
      "  TOV: 0.0042\n",
      "  PF: 0.0046\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Gradient Boosting\n",
      "--------------------------------------------------------------------------------\n",
      "RMSE: 2.2749\n",
      "MAE: 1.6624\n",
      "R²: 0.9333\n",
      "\n",
      "Feature Importances:\n",
      "  MP: 0.0148\n",
      "  FGA: 0.7857\n",
      "  3P: 0.0340\n",
      "  3PA: 0.0098\n",
      "  3P%: 0.0608\n",
      "  FT: 0.0448\n",
      "  FTA: 0.0353\n",
      "  FT%: 0.0108\n",
      "  ORB: 0.0002\n",
      "  DRB: 0.0006\n",
      "  TRB: 0.0027\n",
      "  AST: 0.0002\n",
      "  STL: 0.0001\n",
      "  BLK: 0.0001\n",
      "  TOV: 0.0001\n",
      "  PF: 0.0001\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS FOR PTS\n",
      "================================================================================\n",
      "\n",
      "                          RMSE         MAE        R²\n",
      "Linear Regression     2.174299    1.588467  0.939073\n",
      "Random Forest         2.386818    1.716802  0.926581\n",
      "Gradient Boosting     2.274935    1.662439  0.933303\n",
      "\n",
      "Best Model (by R²): Linear Regression\n",
      "Best Model (by RMSE): Linear Regression\n",
      "Best Model (by MAE): Linear Regression\n"
     ]
    }
   ],
   "source": [
    "# Run complete pipeline for FG prediction\n",
    "pts_results, pts_results_df = predict_target(df, \"PTS\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3000abff-feca-49c0-9145-05ce98d7c393",
   "metadata": {},
   "source": [
    "## 7. Next Steps (To Be Completed)\n",
    "\n",
    "### TODO List for Team:\n",
    "\n",
    "1. **Exploratory Data Analysis (EDA)** - Assigned to: Angel (Week 1-2)\n",
    "   - Distribution plots for PTS (player-level)\n",
    "   - Distribution plots for aggregated team statistics (team-level)\n",
    "   - Correlation heatmaps (both player and team level)\n",
    "   - Win vs Loss feature comparisons\n",
    "   - Temporal trends\n",
    "\n",
    "2. **Feature Engineering** - Assigned to: Ryan + Momoka (Week 1)\n",
    "   - Encode categorical variables (Tm, Opp, Res)\n",
    "   - Create derived features (shooting efficiency, etc.)\n",
    "   - Stretch: Rolling averages for player form\n",
    "   - Stretch goal: PCA (Dimensionality Reduction)\n",
    "\n",
    "3. **Team Win Prediction (Classification)** - Assigned to: **Ryan** (Week 1-2)\n",
    "   - Transform player-level data to team-game level using aggregation\n",
    "   - Binary classification models (Logistic Regression, Random Forest Classifier, Gradient Boosting Classifier)\n",
    "   - Evaluate with accuracy, precision, recall, F1-score, ROC curves\n",
    "   - Compare classification performance across models\n",
    "   - **Deliverable:** New prediction pipeline for binary classification + results comparison\n",
    "\n",
    "4. **Hyperparameter Tuning & Additional Modeling** - Assigned to: Jesus (Week 1-2)\n",
    "   - Implement `tune_hyperparameters()` function with GridSearchCV (cv=5)\n",
    "   - Add XGBoost and LightGBM for both regression (PTS) and classification (Team Win)\n",
    "   - Tune hyperparameters for all models (regression and classification)\n",
    "   - Compare tuned vs baseline models\n",
    "   - **Deliverable:** Tuned models + comparison table\n",
    "     \n",
    "5. **Visualization & Analysis** - Assigned to: Harshil (Week 1-2)\n",
    "   - Residual plots\n",
    "   - Feature importance charts\n",
    "   - Prediction vs actual scatter plots\n",
    "\n",
    "6. **Documentation** - Assigned to: All (Week 2)\n",
    "   - Executive summary (Ryan, Momoka)\n",
    "   - Methodology explanation (Ryan, Jesus)\n",
    "   - Results interpretation (Ryan, Angel, Harshil)\n",
    "   - Conclusions and recommendations (All)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ee8a1-327a-49b4-9c85-d2435344bb5f",
   "metadata": {},
   "source": [
    "## Validity of (3) with our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54d05fc2-b251-4cb3-b52c-bdf396370fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of data to check game structure:\n",
      "                      Player   Tm  Opp        Data Res\n",
      "0               Jayson Tatum  BOS  NYK  2024-10-22   W\n",
      "1              Anthony Davis  LAL  MIN  2024-10-22   W\n",
      "2              Derrick White  BOS  NYK  2024-10-22   W\n",
      "3               Jrue Holiday  BOS  NYK  2024-10-22   W\n",
      "4              Miles McBride  NYK  BOS  2024-10-22   L\n",
      "5              Rui Hachimura  LAL  MIN  2024-10-22   W\n",
      "6               Jaylen Brown  BOS  NYK  2024-10-22   W\n",
      "7                Rudy Gobert  MIN  LAL  2024-10-22   L\n",
      "8              Julius Randle  MIN  LAL  2024-10-22   L\n",
      "9                 Al Horford  BOS  NYK  2024-10-22   W\n",
      "10             Jalen Brunson  NYK  BOS  2024-10-22   L\n",
      "11           Anthony Edwards  MIN  LAL  2024-10-22   L\n",
      "12        Karl-Anthony Towns  NYK  BOS  2024-10-22   L\n",
      "13             Austin Reaves  LAL  MIN  2024-10-22   W\n",
      "14                 Josh Hart  NYK  BOS  2024-10-22   L\n",
      "15  Nickeil Alexander-Walker  MIN  LAL  2024-10-22   L\n",
      "16              LeBron James  LAL  MIN  2024-10-22   W\n",
      "17              Jaxson Hayes  LAL  MIN  2024-10-22   W\n",
      "18             Mikal Bridges  NYK  BOS  2024-10-22   L\n",
      "19             Cameron Payne  NYK  BOS  2024-10-22   L\n",
      "\n",
      "Players per team-game:\n",
      "count    1534.000000\n",
      "mean       10.764016\n",
      "std         1.567064\n",
      "min         8.000000\n",
      "25%        10.000000\n",
      "50%        10.000000\n",
      "75%        12.000000\n",
      "max        15.000000\n",
      "dtype: float64\n",
      "\n",
      "Min players in a team-game: 8\n",
      "Max players in a team-game: 15\n",
      "Average players in a team-game: 10.8\n",
      "\n",
      "Total unique games in dataset: 767\n"
     ]
    }
   ],
   "source": [
    "# Quick verification of data structure\n",
    "print(\"Sample of data to check game structure:\")\n",
    "print(df[['Player', 'Tm', 'Opp', 'Data', 'Res']].head(20))\n",
    "\n",
    "# Check: How many players per team per game?\n",
    "players_per_game = df.groupby(['Tm', 'Opp', 'Data']).size()\n",
    "print(f\"\\nPlayers per team-game:\")\n",
    "print(players_per_game.describe())\n",
    "print(f\"\\nMin players in a team-game: {players_per_game.min()}\")\n",
    "print(f\"Max players in a team-game: {players_per_game.max()}\")\n",
    "print(f\"Average players in a team-game: {players_per_game.mean():.1f}\")\n",
    "\n",
    "# Check: How many unique games?\n",
    "unique_games = df.groupby(['Data']).apply(lambda x: len(x[['Tm', 'Opp']].drop_duplicates()) / 2)\n",
    "print(f\"\\nTotal unique games in dataset: {unique_games.sum():.0f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
