{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# NBA Player Stats Prediction - Refactored\n",
    "\n",
    "**Team Members:** Ryan, Momoka, Jesus, Angel, Harshil \n",
    "**Course:** CS4661 - Introduction to Data Science  \n",
    "**Objective:** Predict NBA player statistics using machine learning\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook demonstrates a complete machine learning pipeline for predicting NBA player statistics:\n",
    "- **Target Variables:** Field Goals (FG) and Field Goal Attempts (FGA)\n",
    "- **Models:** Linear Regression, Random Forest, Gradient Boosting\n",
    "- **Approach:** Modular, reusable functions for scalability and maintainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions",
   "metadata": {},
   "source": [
    "## 2. Reusable Functions\n",
    "\n",
    "These functions eliminate code duplication and make the pipeline modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "helper_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nba_data():\n",
    "    \"\"\"\n",
    "    Download and load NBA player stats dataset from Kaggle.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Raw dataset\n",
    "    \"\"\"\n",
    "    print(\"Downloading dataset...\")\n",
    "    path = kagglehub.dataset_download(\"eduardopalmieri/nba-player-stats-season-2425\")\n",
    "    print(f\"Path to dataset files: {path}\")\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "    print(f\"\\nAvailable CSV files: {csv_files}\")\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(path, csv_files[0]))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATASET OVERVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    print(f\"\\nColumn names:\\n{df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\\n{df.head()}\")\n",
    "    \n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(f\"\\nMissing values:\\n{missing_values[missing_values > 0]}\")\n",
    "    else:\n",
    "        print(\"\\nNo missing values found!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_features(df, target_col, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Prepare features and target variable for modeling.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with raw data\n",
    "        target_col: Name of target variable column\n",
    "        exclude_cols: List of columns to exclude (default: auto-detected)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X, y, feature_names)\n",
    "    \"\"\"\n",
    "    if exclude_cols is None:\n",
    "        # Auto-detect columns to exclude\n",
    "        exclude_cols = [target_col, 'Player', 'Data', 'FG%', 'PTS', 'GmSc']\n",
    "        \n",
    "        # If predicting FG, exclude FGA and vice versa\n",
    "        if target_col == 'FG':\n",
    "            exclude_cols.append('FGA')\n",
    "        elif target_col == 'FGA':\n",
    "            exclude_cols.append('FG')\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Select only numeric features for now\n",
    "    numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nTarget variable: {target_col}\")\n",
    "    print(f\"Feature variables ({len(numeric_cols)} total): {numeric_cols}\")\n",
    "    \n",
    "    # Create feature matrix and target vector\n",
    "    X = df[numeric_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # Clean data\n",
    "    valid_indices = X.notna().all(axis=1) & y.notna()\n",
    "    X = X[valid_indices]\n",
    "    y = y[valid_indices]\n",
    "    \n",
    "    # Handle infinite values\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    y = y[X.index]\n",
    "    \n",
    "    print(f\"Final dataset shape: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    return X, y, numeric_cols\n",
    "\n",
    "\n",
    "def create_model_configs():\n",
    "    \"\"\"\n",
    "    Create model configurations for training.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model configurations\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'Linear Regression': {\n",
    "            'model': LinearRegression(),\n",
    "            'use_scaled': True,\n",
    "            'has_coef': True\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'use_scaled': False,\n",
    "            'has_coef': False\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'model': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "            'use_scaled': False,\n",
    "            'has_coef': False\n",
    "        }\n",
    "    }\n",
    "    return models\n",
    "\n",
    "\n",
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test, feature_cols, target_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate all models for a given target variable.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        X_test: Test features\n",
    "        y_train: Training target\n",
    "        y_test: Test target\n",
    "        feature_cols: List of feature column names\n",
    "        target_name: Name of target variable (for display)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results for each model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"MODEL TRAINING FOR {target_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    models = create_model_configs()\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize scaler once for all models that need it\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    for model_name, config in models.items():\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Select scaled or unscaled data based on model requirements\n",
    "        X_train_use = X_train_scaled if config['use_scaled'] else X_train\n",
    "        X_test_use = X_test_scaled if config['use_scaled'] else X_test\n",
    "        \n",
    "        # Train model\n",
    "        model = config['model']\n",
    "        model.fit(X_train_use, y_train)\n",
    "        y_pred = model.predict(X_test_use)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R²': r2\n",
    "        }\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"R²: {r2:.4f}\")\n",
    "        \n",
    "        # Print coefficients or feature importances\n",
    "        if config['has_coef'] and hasattr(model, 'coef_'):\n",
    "            print(\"\\nFeature Coefficients:\")\n",
    "            for feature, coef in zip(feature_cols, model.coef_):\n",
    "                print(f\"  {feature}: {coef:.4f}\")\n",
    "        elif hasattr(model, 'feature_importances_'):\n",
    "            print(\"\\nFeature Importances:\")\n",
    "            for feature, importance in zip(feature_cols, model.feature_importances_):\n",
    "                print(f\"  {feature}: {importance:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def summarize_results(results, target_name):\n",
    "    \"\"\"\n",
    "    Print summary of model results.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of model results\n",
    "        target_name: Name of target variable\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"SUMMARY OF RESULTS FOR {target_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(f\"\\n{'':<20s}{'RMSE':>10s}{'MAE':>12s}{'R²':>10s}\")\n",
    "    for model_name, row in results_df.iterrows():\n",
    "        print(f\"{model_name:<20s}{row['RMSE']:>10.6f}{row['MAE']:>12.6f}{row['R²']:>10.6f}\")\n",
    "    \n",
    "    # Identify best models\n",
    "    best_model_r2 = results_df['R²'].idxmax()\n",
    "    best_model_rmse = results_df['RMSE'].idxmin()\n",
    "    best_model_mae = results_df['MAE'].idxmin()\n",
    "    \n",
    "    print(f\"\\nBest Model (by R²): {best_model_r2}\")\n",
    "    print(f\"Best Model (by RMSE): {best_model_rmse}\")\n",
    "    print(f\"Best Model (by MAE): {best_model_mae}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def predict_target(df, target_col, test_size=0.4, random_state=42):\n",
    "    \"\"\"\n",
    "    Complete pipeline for predicting a target variable.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with data\n",
    "        target_col: Target variable to predict\n",
    "        test_size: Proportion of data for testing\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results for all models\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(f\"# PREDICTION PIPELINE FOR: {target_col}\")\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    # Prepare features\n",
    "    X, y, feature_cols = prepare_features(df, target_col)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    results = train_and_evaluate_models(\n",
    "        X_train, X_test, y_train, y_test, feature_cols, target_col\n",
    "    )\n",
    "    \n",
    "    # Summarize results\n",
    "    results_df = summarize_results(results, target_col)\n",
    "    \n",
    "    return results, results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Path to dataset files: /Users/ryan/.cache/kagglehub/datasets/eduardopalmieri/nba-player-stats-season-2425/versions/37\n",
      "\n",
      "Available CSV files: ['database_24_25.csv']\n",
      "\n",
      "================================================================================\n",
      "DATASET OVERVIEW\n",
      "================================================================================\n",
      "\n",
      "Dataset shape: (16512, 25)\n",
      "\n",
      "Column names:\n",
      "['Player', 'Tm', 'Opp', 'Res', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'GmSc', 'Data']\n",
      "\n",
      "First few rows:\n",
      "          Player   Tm  Opp Res     MP  FG  FGA    FG%  3P  3PA  ...  DRB  TRB  \\\n",
      "0   Jayson Tatum  BOS  NYK   W  30.30  14   18  0.778   8   11  ...    4    4   \n",
      "1  Anthony Davis  LAL  MIN   W  37.58  11   23  0.478   1    3  ...   13   16   \n",
      "2  Derrick White  BOS  NYK   W  26.63   8   13  0.615   6   10  ...    3    3   \n",
      "3   Jrue Holiday  BOS  NYK   W  30.52   7    9  0.778   4    6  ...    2    4   \n",
      "4  Miles McBride  NYK  BOS   L  25.85   8   10  0.800   4    5  ...    0    0   \n",
      "\n",
      "   AST  STL  BLK  TOV  PF  PTS  GmSc        Data  \n",
      "0   10    1    1    1   1   37  38.1  2024-10-22  \n",
      "1    4    1    3    1   1   36  34.0  2024-10-22  \n",
      "2    4    1    0    0   1   24  22.4  2024-10-22  \n",
      "3    4    1    0    0   2   18  19.5  2024-10-22  \n",
      "4    2    0    0    1   1   22  17.8  2024-10-22  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "\n",
      "No missing values found!\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (only need to do this once!)\n",
    "df = load_nba_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predict_fg",
   "metadata": {},
   "source": [
    "## 4. Predict Field Goals (FG)\n",
    "\n",
    "Field Goals (FG) represents the number of successful shots made by a player in a game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fg_prediction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# PREDICTION PIPELINE FOR: FG\n",
      "################################################################################\n",
      "\n",
      "Target variable: FG\n",
      "Feature variables (15 total): ['MP', '3P', '3PA', '3P%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF']\n",
      "Final dataset shape: X=(16512, 15), y=(16512,)\n",
      "\n",
      "Train set: 9907 samples\n",
      "Test set: 6605 samples\n",
      "\n",
      "================================================================================\n",
      "MODEL TRAINING FOR FG\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Linear Regression\n",
      "--------------------------------------------------------------------------------\n",
      "RMSE: 1.8485\n",
      "MAE: 1.3734\n",
      "R²: 0.6749\n",
      "\n",
      "Feature Coefficients:\n",
      "  MP: 1.1350\n",
      "  3P: 1.5072\n",
      "  3PA: -0.2837\n",
      "  3P%: -0.1100\n",
      "  FT: -0.1143\n",
      "  FTA: 0.6943\n",
      "  FT%: 0.0123\n",
      "  ORB: 0.2277\n",
      "  DRB: 0.1227\n",
      "  TRB: 0.1858\n",
      "  AST: 0.1766\n",
      "  STL: 0.0625\n",
      "  BLK: 0.0646\n",
      "  TOV: 0.2008\n",
      "  PF: -0.1140\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Random Forest\n",
      "--------------------------------------------------------------------------------\n",
      "RMSE: 1.8828\n",
      "MAE: 1.3794\n",
      "R²: 0.6627\n",
      "\n",
      "Feature Importances:\n",
      "  MP: 0.5883\n",
      "  3P: 0.0992\n",
      "  3PA: 0.0227\n",
      "  3P%: 0.0274\n",
      "  FT: 0.0163\n",
      "  FTA: 0.0441\n",
      "  FT%: 0.0157\n",
      "  ORB: 0.0194\n",
      "  DRB: 0.0223\n",
      "  TRB: 0.0357\n",
      "  AST: 0.0307\n",
      "  STL: 0.0171\n",
      "  BLK: 0.0143\n",
      "  TOV: 0.0248\n",
      "  PF: 0.0220\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Gradient Boosting\n",
      "--------------------------------------------------------------------------------\n",
      "RMSE: 1.8276\n",
      "MAE: 1.3402\n",
      "R²: 0.6822\n",
      "\n",
      "Feature Importances:\n",
      "  MP: 0.7073\n",
      "  3P: 0.1349\n",
      "  3PA: 0.0044\n",
      "  3P%: 0.0239\n",
      "  FT: 0.0027\n",
      "  FTA: 0.0644\n",
      "  FT%: 0.0032\n",
      "  ORB: 0.0082\n",
      "  DRB: 0.0021\n",
      "  TRB: 0.0261\n",
      "  AST: 0.0090\n",
      "  STL: 0.0013\n",
      "  BLK: 0.0015\n",
      "  TOV: 0.0100\n",
      "  PF: 0.0010\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS FOR FG\n",
      "================================================================================\n",
      "\n",
      "                          RMSE         MAE        R²\n",
      "Linear Regression     1.848504    1.373403  0.674866\n",
      "Random Forest         1.882773    1.379354  0.662699\n",
      "Gradient Boosting     1.827591    1.340221  0.682181\n",
      "\n",
      "Best Model (by R²): Gradient Boosting\n",
      "Best Model (by RMSE): Gradient Boosting\n",
      "Best Model (by MAE): Gradient Boosting\n"
     ]
    }
   ],
   "source": [
    "# Run complete pipeline for FG prediction\n",
    "fg_results, fg_results_df = predict_target(df, 'FG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predict_fga",
   "metadata": {},
   "source": [
    "## 5. Predict Field Goal Attempts (FGA)\n",
    "\n",
    "Field Goal Attempts (FGA) represents the total number of shots attempted by a player in a game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fga_prediction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# PREDICTION PIPELINE FOR: FGA\n",
      "################################################################################\n",
      "\n",
      "Target variable: FGA\n",
      "Feature variables (15 total): ['MP', '3P', '3PA', '3P%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF']\n",
      "Final dataset shape: X=(16512, 15), y=(16512,)\n",
      "\n",
      "Train set: 9907 samples\n",
      "Test set: 6605 samples\n",
      "\n",
      "================================================================================\n",
      "MODEL TRAINING FOR FGA\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Linear Regression\n",
      "--------------------------------------------------------------------------------\n",
      "RMSE: 2.7500\n",
      "MAE: 2.0331\n",
      "R²: 0.7919\n",
      "\n",
      "Feature Coefficients:\n",
      "  MP: 1.9140\n",
      "  3P: -0.1039\n",
      "  3PA: 2.9191\n",
      "  3P%: -0.1271\n",
      "  FT: 0.0386\n",
      "  FTA: 1.0450\n",
      "  FT%: -0.0554\n",
      "  ORB: 0.5157\n",
      "  DRB: 0.1077\n",
      "  TRB: 0.2872\n",
      "  AST: 0.4037\n",
      "  STL: 0.0830\n",
      "  BLK: 0.0410\n",
      "  TOV: 0.3919\n",
      "  PF: -0.1855\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Random Forest\n",
      "--------------------------------------------------------------------------------\n",
      "RMSE: 2.8018\n",
      "MAE: 2.0417\n",
      "R²: 0.7840\n",
      "\n",
      "Feature Importances:\n",
      "  MP: 0.6348\n",
      "  3P: 0.0057\n",
      "  3PA: 0.1662\n",
      "  3P%: 0.0109\n",
      "  FT: 0.0144\n",
      "  FTA: 0.0308\n",
      "  FT%: 0.0110\n",
      "  ORB: 0.0162\n",
      "  DRB: 0.0153\n",
      "  TRB: 0.0200\n",
      "  AST: 0.0222\n",
      "  STL: 0.0113\n",
      "  BLK: 0.0084\n",
      "  TOV: 0.0176\n",
      "  PF: 0.0152\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Gradient Boosting\n",
      "--------------------------------------------------------------------------------\n",
      "RMSE: 2.7241\n",
      "MAE: 1.9857\n",
      "R²: 0.7958\n",
      "\n",
      "Feature Importances:\n",
      "  MP: 0.6794\n",
      "  3P: 0.0009\n",
      "  3PA: 0.2250\n",
      "  3P%: 0.0012\n",
      "  FT: 0.0041\n",
      "  FTA: 0.0468\n",
      "  FT%: 0.0011\n",
      "  ORB: 0.0100\n",
      "  DRB: 0.0010\n",
      "  TRB: 0.0093\n",
      "  AST: 0.0105\n",
      "  STL: 0.0006\n",
      "  BLK: 0.0007\n",
      "  TOV: 0.0087\n",
      "  PF: 0.0007\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS FOR FGA\n",
      "================================================================================\n",
      "\n",
      "                          RMSE         MAE        R²\n",
      "Linear Regression     2.749998    2.033133  0.791892\n",
      "Random Forest         2.801822    2.041688  0.783975\n",
      "Gradient Boosting     2.724104    1.985679  0.795793\n",
      "\n",
      "Best Model (by R²): Gradient Boosting\n",
      "Best Model (by RMSE): Gradient Boosting\n",
      "Best Model (by MAE): Gradient Boosting\n"
     ]
    }
   ],
   "source": [
    "# Run complete pipeline for FGA prediction\n",
    "fga_results, fga_results_df = predict_target(df, 'FGA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## 6. Side-by-Side Comparison\n",
    "\n",
    "Compare model performance across both prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "compare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON: FG vs FGA PREDICTION\n",
      "================================================================================\n",
      "\n",
      "---------------------------------------- FG Results ----------------------------------------\n",
      "                       RMSE       MAE        R²\n",
      "Linear Regression  1.848504  1.373403  0.674866\n",
      "Random Forest      1.882773  1.379354  0.662699\n",
      "Gradient Boosting  1.827591  1.340221  0.682181\n",
      "\n",
      "---------------------------------------- FGA Results ----------------------------------------\n",
      "                       RMSE       MAE        R²\n",
      "Linear Regression  2.749998  2.033133  0.791892\n",
      "Random Forest      2.801822  2.041688  0.783975\n",
      "Gradient Boosting  2.724104  1.985679  0.795793\n",
      "\n",
      "================================================================================\n",
      "KEY INSIGHTS\n",
      "================================================================================\n",
      "Best model for FG: Gradient Boosting (R² = 0.6822)\n",
      "Best model for FGA: Gradient Boosting (R² = 0.7958)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: FG vs FGA PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*40 + \" FG Results \" + \"-\"*40)\n",
    "print(fg_results_df)\n",
    "\n",
    "print(\"\\n\" + \"-\"*40 + \" FGA Results \" + \"-\"*40)\n",
    "print(fga_results_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best model for FG: {fg_results_df['R²'].idxmax()} (R² = {fg_results_df['R²'].max():.4f})\")\n",
    "print(f\"Best model for FGA: {fga_results_df['R²'].idxmax()} (R² = {fga_results_df['R²'].max():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "## 7. Next Steps (To Be Completed)\n",
    "\n",
    "### TODO List for Team:\n",
    "\n",
    "1. **Exploratory Data Analysis (EDA)** - Assigned to: Angel\n",
    "   - Distribution plots for FG and FGA\n",
    "   - Correlation heatmap\n",
    "   - Feature relationships\n",
    "   - Temporal trends\n",
    "\n",
    "2. **Feature Engineering** - Assigned to: Ryan\n",
    "   - Encode categorical variables (Tm, Opp, Res)\n",
    "   - Create derived features (shooting efficiency, etc.)\n",
    "   - Rolling averages for player form\n",
    "\n",
    "3. **Advanced Modeling** - Assigned to: Jesus\n",
    "   - Cross-validation (5-fold)\n",
    "   - Hyperparameter tuning\n",
    "   - Add XGBoost and LightGBM\n",
    "   - Feature selection\n",
    "\n",
    "4. **Visualization & Analysis** - Assigned to: \n",
    "   - Residual plots\n",
    "   - Feature importance charts\n",
    "   - Prediction vs actual scatter plots\n",
    "\n",
    "5. **Documentation** - Assigned to: Momoka\n",
    "   - Executive summary\n",
    "   - Methodology explanation\n",
    "   - Results interpretation\n",
    "   - Conclusions and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3f8e1-475a-437a-8b2d-91578f18098a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
